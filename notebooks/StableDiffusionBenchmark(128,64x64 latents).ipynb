{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Generate 128, 64x64 random latent vectors - pass into auto encoder - time it","metadata":{}},{"cell_type":"code","source":"# clone the repository\n!git clone https://github.com/kk-digital/kcg-ml-sd1p4.git\n# move to the repo\n%cd kcg-ml-sd1p4/","metadata":{"execution":{"iopub.status.busy":"2023-04-04T14:25:57.985199Z","iopub.execute_input":"2023-04-04T14:25:57.985615Z","iopub.status.idle":"2023-04-04T14:25:59.679801Z","shell.execute_reply.started":"2023-04-04T14:25:57.985556Z","shell.execute_reply":"2023-04-04T14:25:59.678458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Install requirements \n!pip install diffusers==0.11.1\n!pip install transformers scipy ftfy accelerate\n!pip3 install labml\n!pip3 install labml-nn\n!pip3 install pytorch-lightning","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-04T14:24:45.222698Z","iopub.execute_input":"2023-04-04T14:24:45.223671Z","iopub.status.idle":"2023-04-04T14:25:57.982513Z","shell.execute_reply.started":"2023-04-04T14:24:45.223629Z","shell.execute_reply":"2023-04-04T14:25:57.981244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# download model weights\n!wget https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4.ckpt","metadata":{"execution":{"iopub.status.busy":"2023-04-04T14:25:59.68259Z","iopub.execute_input":"2023-04-04T14:25:59.683318Z","iopub.status.idle":"2023-04-04T14:26:18.694009Z","shell.execute_reply.started":"2023-04-04T14:25:59.683282Z","shell.execute_reply":"2023-04-04T14:26:18.692735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n---\ntitle: Generate images using stable diffusion with a prompt\nsummary: >\n Generate images using stable diffusion with a prompt\n---\n\n# Generate images using [stable diffusion](../index.html) with a prompt\n\"\"\"\n\nimport os\nfrom pathlib import Path\n\nimport torch\n\nfrom labml import lab, monit\nfrom stable_diffusion.latent_diffusion import LatentDiffusion\nfrom stable_diffusion.sampler.ddim import DDIMSampler\nfrom stable_diffusion.sampler.ddpm import DDPMSampler\nfrom stable_diffusion.util import load_model, save_images, set_seed\n\n\nclass Txt2Img:\n    \"\"\"\n    ### Text to image class\n    \"\"\"\n    model: LatentDiffusion\n\n    def __init__(self, *,\n                 checkpoint_path: Path,\n                 sampler_name: str,\n                 n_steps: int = 50,\n                 ddim_eta: float = 0.0,\n                 ):\n        \"\"\"\n        :param checkpoint_path: is the path of the checkpoint\n        :param sampler_name: is the name of the [sampler](../sampler/index.html)\n        :param n_steps: is the number of sampling steps\n        :param ddim_eta: is the [DDIM sampling](../sampler/ddim.html) $\\eta$ constant\n        \"\"\"\n        # Load [latent diffusion model](../latent_diffusion.html)\n        self.model = load_model(checkpoint_path)\n        # Get device\n        self.device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n        # Move the model to device\n        self.model.to(self.device)\n\n        # Initialize [sampler](../sampler/index.html)\n        if sampler_name == 'ddim':\n            self.sampler = DDIMSampler(self.model,\n                                       n_steps=n_steps,\n                                       ddim_eta=ddim_eta)\n        elif sampler_name == 'ddpm':\n            self.sampler = DDPMSampler(self.model)\n\n    @torch.no_grad()\n    def __call__(self, *,\n                 dest_path: str,\n                 batch_size: int = 3,\n                 prompt: str,\n                 h: int = 512, w: int = 512,\n                 uncond_scale: float = 7.5,\n                 ):\n        \"\"\"\n        :param dest_path: is the path to store the generated images\n        :param batch_size: is the number of images to generate in a batch\n        :param prompt: is the prompt to generate images with\n        :param h: is the height of the image\n        :param w: is the width of the image\n        :param uncond_scale: is the unconditional guidance scale $s$. This is used for\n            $\\epsilon_\\theta(x_t, c) = s\\epsilon_\\text{cond}(x_t, c) + (s - 1)\\epsilon_\\text{cond}(x_t, c_u)$\n        \"\"\"\n        # Number of channels in the image\n        c = 4\n        # Image to latent space resolution reduction\n        f = 8\n\n        # Make a batch of prompts\n        prompts = batch_size * [prompt]\n\n        # AMP auto casting\n        with torch.cuda.amp.autocast():\n            # In unconditional scaling is not $1$ get the embeddings for empty prompts (no conditioning).\n            if uncond_scale != 1.0:\n                un_cond = self.model.get_text_conditioning(batch_size * [\"\"])\n            else:\n                un_cond = None\n            # Get the prompt embeddings\n            cond = self.model.get_text_conditioning(prompts)\n            # [Sample in the latent space](../sampler/index.html).\n            # `x` will be of shape `[batch_size, c, h / f, w / f]`\n            x = self.sampler.sample(cond=cond,\n                                    shape=[batch_size, c, h // f, w // f],\n                                    uncond_scale=uncond_scale,\n                                    uncond_cond=un_cond)\n            # Decode the image from the [autoencoder](../model/autoencoder.html)\n            images = self.model.autoencoder_decode(x)\n\n        # Save images\n        save_images(images, dest_path, 'txt_')\n\n    # functions for pipeline\n    @torch.no_grad()\n    def generate_text_embeddings(self, prompt, batch_size=4, uncond_scale=7.5):\n        \"\"\"\n        :param prompt: is the prompt to generate images with\n        \"\"\"\n        # Make a batch of prompts\n        prompts = batch_size * [prompt]\n\n        # AMP auto casting\n        with torch.cuda.amp.autocast():\n            # In unconditional scaling is not $1$ get the embeddings for empty prompts (no conditioning).\n            if uncond_scale != 1.0:\n                un_cond = self.model.get_text_conditioning(batch_size * [\"\"])\n            else:\n                un_cond = None\n            # Get the prompt embeddings\n            cond = self.model.get_text_conditioning(prompts)\n        \n        # return the embeddings\n        return cond, un_cond\n    \n    @torch.no_grad()\n    def generate_latent_space(self, cond, un_cond, batch_size=4, uncond_scale=7.5, h=512, w=512):\n        \"\"\"\n        :param prompt: is the prompt to generate images with\n        \"\"\"\n        # Number of channels in the image\n        c = 4\n        # Image to latent space resolution reduction\n        f = 8\n\n        # AMP auto casting\n        with torch.cuda.amp.autocast():\n            # [Sample in the latent space](../sampler/index.html).\n            # `x` will be of shape `[batch_size, c, h / f, w / f]`\n            x = self.sampler.sample(cond=cond,\n                                    shape=[batch_size, c, h // f, w // f],\n                                    uncond_scale=uncond_scale,\n                                    uncond_cond=un_cond)\n        \n        # return the embeddings\n        return x\n    \n    @torch.no_grad()\n    def generate_image(self, x):\n        \"\"\"\n        :param prompt: is the prompt to generate images with\n        \"\"\"\n        # AMP auto casting\n        with torch.cuda.amp.autocast():\n            # Decode the image from the [autoencoder](../model/autoencoder.html)\n            image = self.model.autoencoder_decode(x)\n        \n        # return the embeddings\n        return image\n","metadata":{"execution":{"iopub.status.busy":"2023-04-04T14:26:18.698541Z","iopub.execute_input":"2023-04-04T14:26:18.698951Z","iopub.status.idle":"2023-04-04T14:26:31.500876Z","shell.execute_reply.started":"2023-04-04T14:26:18.698904Z","shell.execute_reply":"2023-04-04T14:26:31.499708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Return 128 image & Calculate time","metadata":{}},{"cell_type":"code","source":"import time\nimport torch\n\n# Initialize the Txt2Img class\ntxt2img = Txt2Img(checkpoint_path='sd-v1-4.ckpt',\n                  sampler_name='ddim',\n                  n_steps=50,\n                  ddim_eta=0.0)\n\n# Define parameters for generating images\n# N determines how many images will be generated in a single pass\nN = 128\nh = 64\nw = 64\nc = 4\nf = 8\n\n# Create random latent vectors\nlatent_vectors = torch.randn(N, c, h // f, w // f).to(txt2img.device)\n\n# Time the decoding process\nstart_time = time.time()\n\n# Decode images from latent vectors\n\nimages = txt2img.generate_image(latent_vectors)\n\n# Calculate and print the time taken\nend_time = time.time()\ntime_elapsed = end_time - start_time\ntime_per_image = time_elapsed / N\nprint(f\"number of images: {N}\")\nprint(f\"Time taken to generate images: {time_elapsed:.4f} seconds\")\nprint(f\"Time taken per image: {time_per_image:.4f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2023-04-04T14:26:31.502747Z","iopub.execute_input":"2023-04-04T14:26:31.503115Z","iopub.status.idle":"2023-04-04T14:27:34.526457Z","shell.execute_reply.started":"2023-04-04T14:26:31.503063Z","shell.execute_reply":"2023-04-04T14:27:34.525196Z"},"trusted":true},"execution_count":null,"outputs":[]}]}